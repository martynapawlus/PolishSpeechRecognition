# TEST ROZPOZNAWANIA JZYKA POLSKIEGO
***by Martyna, Julia, Karolina***

Proces ma na celu zweryfikowanie ilociowej skutecznoci dziaania silnik贸w rozpoznawania mowy w jzku polskim, poprzez przeprowadzenie test贸w na zebranym zbiorze danych.
<br />Etapy przeprowadzania test贸w:
1. Przygotowanie nagra (zar贸wno pobranych z internetu jak i nagranych) i podzielenie ich na sety. Wszystkie wykorzystane do testowania pliki audio znajduj si w repozytorium.
2. Opracowanie plik贸w z transkrypcj do wykorzystania jako teksty referencyjne.
3. Przepuszczenie plik贸w audio przez wybrane silniki rozpoznawania mowy.
    * [Google Cloud Platform](https://cloud.google.com/speech-to-text/docs/reference/rest/)
    * [Google Speech Recognition](https://github.com/Uberi/speech_recognition/blob/master/reference/library-reference.rst#recognizer_instancerecognize_googleaudio_data-audiodata-key-unionstr-none--none-language-str--en-us--pfilter-union0-1-show_all-bool--false---unionstr-dictstr-any)
    * [Azure](https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text)
    * [Happyscribe](https://www.happyscribe.com/transcribe-polish)
    * [Sonix](https://sonix.ai/) <br />
*Pene testy zostay przeprowadzone na wszystkich wy偶ej wymienionych z wyjtkiem Google Speech Recognition* <br />

Skrypty usprawniajce prac z silnikami r贸wnie偶 znajduj si w repozytorium w folderze [scripts](https://github.com/martynapawlus/PolishSpeechRecognition/tree/main/scripts).

4. Obliczenie **Word Error Rate (WER)** przy u偶yciu programu [sclite](https://github.com/usnistgov/SCTK).
5. Przygotowanie pliku [.csv](https://github.com/martynapawlus/PolishSpeechRecognition/tree/main/wyniki) zawierajcego w kolumnach wszystkie cechy poszczeg贸lnych tekst贸w i nagra. 
6. Analiza wynik贸w w rodowisku R.

### Podsumowanie
Opr贸cz wykorzystywanego narzdzia transkypcji, rozwa偶any by wpyw nastpujcych czynnik贸w:
  * dykcja
  * jako mikrofonu
  * pe
  * typ sownictwa
  * szum ta
  * szybko mowy
<br /> <br />Najbardziej znaczcym z czynnik贸w okazaa si dykcja oraz du偶y szum. <br />
<img src="https://github.com/martynapawlus/PolishSpeechRecognition/blob/main/wykresy/10.png" alt="ten" width="600"/>
W przypadku os贸b z dobr dykcj rednia warto WER wyniosa 19.11%, natomiast dla os贸b ze z dykcj warto ta bya na poziomie 58.74% P-value dla tego testu osigno warto 2e-16, wic byo bliskie zeru. Wpyw dykcji na odczyt WER okaza si niezaprzeczalny. Por贸wnanie  grupy  os贸b  z  dobr  oraz  przecitn  dykcj  dostarczyo  spodziewanie  mniejszej  r贸偶nicy ni偶 w przypadku powy偶szego zestawienia, ale nadal to transkrypcje os贸b z dobr dykcj osigny widocznie lepsze rezultaty. P-value w tym przypadku ma warto 0.0121.Prawdopodobiestwo, 偶e r贸偶nica midzy dwoma przedziaami jest zjawiskiem losowym jest wic bardzo mae.
<br />
<img src="https://github.com/martynapawlus/PolishSpeechRecognition/blob/main/wykresy/2.png" alt="two" width="650"/>
Du偶y szum okaza si mie r贸wnie偶 nieprzypadkowy wpyw na wyniki WER, poniewa偶 w przypadku tego zestawienia warto P-value wyniosa zaledwie 0.00146.

Pozostae czynniki okazay si mao znaczce, a uzyskane wartoci P-value sugeroway losowo wynik贸w:
  * **Jako mikrofonu** - r贸偶nica w wartociach WER midzy dobrym, a zym mikrofonem wynosiosa okoo 1 punkta procentowego. 
  * **Pe** - r贸偶nica w wartociach WER dla lektor贸w r贸偶nych pci wyniosa 0.4 pp.
  * **Typ sownictwa** - tutaj rozbie偶no wartoci WER bya nieco wiksza, bo ponad 2 pp, ale od strony statystycznej por贸wnanie to nie ma du偶ego znaczenia.
  * **May szum ta** - r贸偶nica midzy maym szumem, a brakiem szumu bya na poziomie ok. 1pp, wic wnioski nasuwaj si same.

**Midzy poszczeg贸lnymi silnikami wystpiy widoczne i nieprzypadkowe r贸偶nice w wartociach WER.**

<img src="https://github.com/martynapawlus/PolishSpeechRecognition/blob/main/wykresy/6.png" alt="two" width="700"/>

Do oblicze potrzebnych do stworzenia tego zestawienia wykorzystane zostay wszystkie przygotowane nagrania, bez wzgldu na badane czynniki, takie jak jako mowy i mikrofonu. Ka偶dy z silnik贸w przeprowadza transkrypcj dokadnie takiego samego zestawu nagra. Raport nie ujawnia nazw narzdzi, kt贸re uzyskay poszczeg贸lne wyniki. Prezentuj si one natomiast nastpujco:
  * **Narzdzie 1** - redni WER: 23.47%
  * **Narzdzie 2** - redni WER: 30.30.38%
  * **Narzdzie 3** - redni WER: 36.31%
  * **Narzdzie 4** - redni WER: 34.83%

Co ciekawe, Google Speech Recognition (inne API), mimo tego, 偶e teoretycznie korzysta z tego samego silnika, dao inne rezultaty ni偶 Google Cloud Platform. Wikszo nagra zostaa przetranskrybowana nieco lepiej przy u偶yciu GSR.
<img src="https://github.com/martynapawlus/PolishSpeechRecognition/blob/main/wykresy/4.png" alt="two" width="600"/>

redni WER dla Google Speech Recognition wyni贸s - 17.72%, a dla Google Cloud Platform - 21.15%. Obliczenia te byy przeprowadzone tylko dla czci tekst贸w, poniewa偶 GSR nie podejmowa pr贸by transkrypcji nagra, kt贸re charaktekryzoway si sab dykcj lub du偶ym szumem. Zamiast tego wyrzuca bd "UnknownValueError".

Testy, kt贸re wykonaymy day satysfakcjonujce wyniki, zwaszcza w kwestii pr贸wnania komercyjnych silnik贸w transkrypcji mowy.
